{
    "summary": "The Gato class in Keras processes inputs with different encodings using components like image_embedding and transformer. The code initializes a Transformer model, performs value masking, adds local position encodings, and defines PatchEmbedding for residual embedding.",
    "details": [
        {
            "comment": "The code defines a Gato class that inherits from Keras Model. It takes a configuration and optional trainable and name parameters for initialization. The class initializes various components such as image_embedding, discrete_embedding, continuous_encoding, and transformer based on the provided config.",
            "location": "\"/media/root/Prima/works/gato/docs/src/gato/models/__init__.py\":0-21",
            "content": "import tensorflow as tf\nfrom gato.models.transformer import TransformerBlock\nfrom gato.models.embedding import PatchPositionEncoding, ResidualEmbedding, LocalPositionEncoding, DiscreteEmbedding\nfrom gato.models.tokenizers import ContinuousValueTokenizer\nfrom tensorflow.keras import models\nfrom gato import GatoConfig\nfrom typing import Dict, Any, Union\nclass Gato(models.Model):\n    def __init__(self, config: Union[GatoConfig, Dict[str, Any]], trainable: bool = True, name: str = 'Gato', **kwargs):\n        super(Gato, self).__init__(trainable=trainable, name=name, **kwargs)\n        if isinstance(config, dict):\n            config = GatoConfig(**config)\n        self.config = config\n        self.image_embedding = PatchEmbedding(config, trainable=trainable, name='ImagePatchEmbedding')\n        self.discrete_embedding = DiscreteEmbedding(config, trainable=trainable, name='DiscreteEmbedding')\n        self.continuous_encoding = ContinuousValueTokenizer(config, name='ContinuousValueEncoding')\n        self.transformer = Transformer(config, trainable=trainable, name='Transformers')"
        },
        {
            "comment": "This code snippet is defining a model class for processing inputs with various types of encoding (image, continuous, discrete). It creates a local position encoding and applies one-hot encoding to the input. The image_embedding function is used to extract image embeddings, which are then masked using the image patch mask derived from the one-hot encoded input.",
            "location": "\"/media/root/Prima/works/gato/docs/src/gato/models/__init__.py\":22-41",
            "content": "        self.local_pos_encoding = LocalPositionEncoding(config, trainable=trainable, name='LocalPositionEncoding')\n    def call(self, inputs, training=None, mask=None):\n        # input_ids with (B, L, 768)\n        # encoding with (B, L) or (B,)\n        # row_pos and col_pos with tuple of (pos_from, pos_to)\n        # obs_pos and obs_mask with (B, L) or (B,)\n        input_ids, (encoding, row_pos, col_pos), (obs_pos, obs_mask) = inputs\n        # Encoding flags for embed masks\n        # 0 - image\n        # 1 - continuous\n        # 2 - discrete (actions, texts)\n        encoding = tf.one_hot(encoding, depth=3, dtype=tf.float32)\n        ones = tf.ones((input_ids.shape[0], 1, self.config.layer_width), dtype=tf.float32)\n        image_embed = self.image_embedding((input_ids, (row_pos, col_pos)), training=training)\n        image_embed *= encoding[..., 0].transpose().matmul(ones)  # image patch masking\n        # continuous value takes from first value of input_ids\n        continuous_embed = self.continuous_encoding(input_ids[..., 0])"
        },
        {
            "comment": "The code initializes a Transformer model in the Gato architecture. It performs value masking for continuous and discrete values, adds local observation position encodings, and passes the input through a transformer layer.",
            "location": "\"/media/root/Prima/works/gato/docs/src/gato/models/__init__.py\":42-67",
            "content": "        continuous_embed = self.discrete_embedding(continuous_embed)\n        continuous_embed *= encoding[..., 1].transpose().matmul(ones)  # continuous value masking\n        discrete_embed = self.discrete_embedding(input_ids[..., 0])\n        discrete_embed *= encoding[..., 2].transpose().matmul(ones)  # discrete value masking\n        # Appendix C.3. Position Encodings > Local Observation Position Encodings\n        # add local observation position encodings\n        embed = image_embed + continuous_embed + discrete_embed\n        embed += self.local_pos_encoding((obs_pos, obs_mask))\n        hidden_states = self.transformer(embed)\n        return hidden_states\n    def get_config(self):\n        return super(Gato, self).get_config()\nclass Transformer(models.Model):\n    def __init__(self,\n                 config: Union[GatoConfig, Dict[str, Any]],\n                 trainable: bool = True,\n                 name: str = None,\n                 **kwargs):\n        super(Transformer, self).__init__(trainable=trainable, name=name, **kwargs)"
        },
        {
            "comment": "This code initializes an instance of the Transformer class, which contains a list of TransformerBlock objects. It also defines a PatchEmbedding class for residual embedding. The config parameter is checked and converted to a GatoConfig object if needed.",
            "location": "\"/media/root/Prima/works/gato/docs/src/gato/models/__init__.py\":68-95",
            "content": "        if isinstance(config, dict):\n            config = GatoConfig(**config)\n        self.config = config\n        self.encoders = [TransformerBlock(config=self.config, trainable=trainable, name='EncoderBlock{}'.format(idx))\n                         for idx in range(self.config.num_transformer_blocks)]\n    def call(self, inputs, training=None, mask=None):\n        x = inputs\n        for encoder in self.encoders:\n            x = encoder(x)\n        return x\n    def get_config(self):\n        return super(Transformer, self).get_config()\nclass PatchEmbedding(models.Model):\n    def __init__(self,\n                 config: Union[GatoConfig, Dict[str, Any]],\n                 trainable: bool = True,\n                 name: str = None,\n                 **kwargs):\n        super(PatchEmbedding, self).__init__(trainable=trainable, name=name, **kwargs)\n        if isinstance(config, dict):\n            config = GatoConfig(**config)\n        self.config = config\n        self.residual_embedding = ResidualEmbedding(config, trainable=trainable, name='ResidualEmbedding')"
        },
        {
            "comment": "This code defines a PatchEmbedding class that performs patch-based embedding for image data. It initializes a PatchPositionEncoding object, reshapes the input, applies residual embedding, and then passes it through positional encoding based on (row_pos, col_pos). The get_config method returns the configuration.",
            "location": "\"/media/root/Prima/works/gato/docs/src/gato/models/__init__.py\":96-109",
            "content": "        self.pos_encoding = PatchPositionEncoding(config, trainable=trainable, name='PatchPositionEncoding')\n    def call(self, inputs, training=None, mask=None):\n        input_ids, (row_pos, col_pos) = inputs\n        patch_size = self.config.img_patch_size\n        depth = self.config.input_dim // (patch_size * patch_size)\n        x = input_ids.reshape((-1, input_ids.shape[1], patch_size, patch_size, depth))\n        x = self.residual_embedding(x)\n        x = self.pos_encoding((x, (row_pos, col_pos)))\n        return x\n    def get_config(self):\n        return super(PatchEmbedding, self).get_config()"
        }
    ]
}