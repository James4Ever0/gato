{
    "summary": "This unofficial Gato implementation includes a Transformer and Embedding Function, is in development and requires datasets for training. It has variations in architecture including Residual Embedding layer and Vision Transformer-like position encodings, with TensorFlow 2.11.0 or higher needed to run the open-source project licensed under MIT.",
    "details": [
        {
            "comment": "This README introduces an unofficial implementation of Gato, a generalist agent architecture by Deepmind. The code contains models like Gato, Transformer, Patch Position Encodings, and Embedding Function. The repository is still under development and supports the mentioned operations.",
            "location": "\"/media/root/Prima/works/gato/docs/src/README.md\":0-14",
            "content": "<h1 align=\"center\">Unofficial Gato: A Generalist Agent</h1>\n[[Deepmind Publication]](https://www.deepmind.com/publications/a-generalist-agent)\n[[arXiv Paper]](https://arxiv.org/pdf/2205.06175.pdf)\nThis repository contains Deepmind's Gato architecture imitation in TensorFlow.\nSince Deepmind only mentions parts of the architecture in its paper, We still don't know much about the model.<br>\nHowever, I believe the paper is enough to imitate the architecture, I'm trying to do that with the open source community's help.\nCurrently, the repository supports the following operations:\n- Gato (via [`Gato`](https://github.com/OrigamiDream/gato/blob/main/gato/models/__init__.py#L12))\n- Transformer (via [`Transformer`](https://github.com/OrigamiDream/gato/blob/main/gato/models/__init__.py#L61))\n- Patch Position Encodings (via [`PatchPositionEncoding`](https://github.com/OrigamiDream/gato/blob/main/gato/models/embedding.py#L38))\n- Embedding Function (via [`ResidualEmbedding`](https://github.com/OrigamiDream/gato/blob/main/gato/models/embedding.py#L139))"
        },
        {
            "comment": "Code snippet from gato/README.md:\nThe code introduces the basic architecture of Gato, its tokenizing mechanisms, and embedding layers. It also mentions the lack of pre-trained tokenizers due to an E2E model, and the need for datasets which are still in development. The code provides installation instructions using pip and importing Gato module in TensorFlow.",
            "location": "\"/media/root/Prima/works/gato/docs/src/README.md\":15-34",
            "content": "- Local Observation Position Encodings (via [`LocalPositionEncoding`](https://github.com/OrigamiDream/gato/blob/main/gato/models/embedding.py#L199))\n- Tokenizing Continuous Values (via [`ContinuousValueTokenizer`](https://github.com/OrigamiDream/gato/blob/main/gato/models/tokenizers.py#L30))\n- Shared Embedding (via [`DiscreteEmbedding`](https://github.com/OrigamiDream/gato/blob/main/gato/models/embedding.py#L237))\nAction tokens are still a mystery in the paper, I need your help.\nHowever, the repository lacks the following miscellaneous.\n- Datasets (most important, Issue: [#1](https://github.com/OrigamiDream/gato/issues/1), [ThomasRochefortB/torch-gato](https://github.com/ThomasRochefortB/torch-gato/blob/main/datasets/README.md))\n- <s>Pre-trained tokenizers</s> (No longer required because of E2E model)\n- Training strategy (E2E, WIP)\nBut, you can still explore the basic architecture of the Gato based on the paper.\n### Usage\n```bash\n$ pip install gato-tf\n```\n```python\nimport tensorflow as tf\nfrom gato import Gato, GatoConfig"
        },
        {
            "comment": "Creates a Gato model instance using the provided configuration and generates fake inputs for the model. The inputs include 20 image patches, two continuous values, and two discrete (actions, texts) observations. Image patches have random uniform distributions while continuous and discrete values are filled with specific values. An embedding constant is created for different input types.",
            "location": "\"/media/root/Prima/works/gato/docs/src/README.md\":36-65",
            "content": "# Create model instance\nconfig = GatoConfig.small()\ngato = Gato(config)\n# Fake inputs for Gato\ninput_dim = config.input_dim\ninput_ids = tf.concat([\n  # ...\n  # observation 1\n  tf.random.uniform((1, 1, input_dim)),  # image patch 0\n  tf.random.uniform((1, 1, input_dim)),  # image patch 1\n  tf.random.uniform((1, 1, input_dim)),  # image patch 2\n  # ...\n  tf.random.uniform((1, 1, input_dim)),  # image patch 19\n  tf.fill((1, 1, input_dim), value=0.25),  # continuous value\n  tf.fill((1, 1, input_dim), value=624.0),  # discrete (actions, texts)\n  # observation 2\n  tf.random.uniform((1, 1, input_dim)),  # image patch 0\n  tf.random.uniform((1, 1, input_dim)),  # image patch 1\n  tf.random.uniform((1, 1, input_dim)),  # image patch 2\n  # ...\n  tf.random.uniform((1, 1, input_dim)),  # image patch 19\n  tf.fill((1, 1, input_dim), value=0.12),  # continuous value\n  tf.fill((1, 1, input_dim), value=295.0)  # discrete (actions, texts)\n  # ...\n], axis=1)\nencoding = tf.constant([\n  # 0 - image patch embedding\n  # 1 - continuous value embedding"
        },
        {
            "comment": "The code is initializing tensorflow constants for input ids, encoding, row positions, column positions, and observation tokens. It then passes these to the GATO model for hidden states calculation. The GATO model is a multi-modal transformer-based architecture for vision-language-task (VLT) understanding. The dataset consists of images with associated text descriptions which are fed into the model for prediction or task completion.",
            "location": "\"/media/root/Prima/works/gato/docs/src/README.md\":66-86",
            "content": "  # 2 - discrete embedding (actions, texts)\n  [0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 2]\n])\nrow_pos = (\n  tf.constant([[0.00, 0.25, 0.50, 0.75, 0, 0, 0.00, 0.25, 0.50, 0.75, 0, 0]]),  # pos_from\n  tf.constant([[0.25, 0.50, 0.75, 1.00, 0, 0, 0.25, 0.50, 0.75, 1.00, 0, 0]])   # pos_to\n)\ncol_pos = (\n  tf.constant([[0.00, 0.00, 0.00, 0.80, 0, 0, 0.00, 0.00, 0.00, 0.80, 0, 0]]),  # pos_from\n  tf.constant([[0.20, 0.20, 0.20, 1.00, 0, 0, 0.20, 0.20, 0.20, 1.00, 0, 0]])   # pos_to\n)\nobs = (\n  tf.constant([[ 0,  1,  2, 19, 20, 21,  0,  1,  2, 19, 20, 21]]),  # obs token\n  tf.constant([[ 1,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  0]])   # obs token masking (for action tokens)\n)\nhidden_states = gato((input_ids, (encoding, row_pos, col_pos), obs))\n```\n### Dataset and Model Architecture\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://user-images.githubusercontent.com/5837620/215323793-7f7bcfdb-d8be-40d3-8e58-a053511f95d5.png\">\n  <img alt=\"gato dataset and model architecture\" src=\"https://user-images.githubusercontent.com/5837620/215323795-3a433516-f5ca-4272-9999-3df87ae521ba.png\">"
        },
        {
            "comment": "The code describes the architecture variants of Gato used in the paper, specifically the Large (1.18B), Baseline (364M), and Small (79M) versions. These are named as `large()`, `baseline()`, and `small()` respectively in `GatoConfig`. The table shows the differences in hyperparameters such as transformer blocks, attention heads, layer width, and feedforward hidden size for each variant.",
            "location": "\"/media/root/Prima/works/gato/docs/src/README.md\":87-110",
            "content": "</picture>\n## Paper Reviews\n### Full Episode Sequence\n<picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://user-images.githubusercontent.com/5837620/175756389-31d183c9-054e-4829-93a6-df79781ca212.png\">\n    <img alt=\"gato dataset architecture\" src=\"https://user-images.githubusercontent.com/5837620/175756409-75605dbc-7756-4509-ba93-c0ad08eea309.png\">\n</picture>\n### Architecture Variants\n> Appendix C.1. Transformer Hyperparameters\nIn the paper, Deepmind tested Gato with 3 architecture variants, `1.18B`, `364M`, and `79M`.<br>\nI have named them as `large()`, `baseline()` and `small()` respectively in `GatoConfig`.\n| Hyperparameters          | Large(1.18B) | Baseline(364M) | Small(79M) |\n|--------------------------|--------------|----------------|------------|\n| Transformer blocks       | 24           | 12             | 8          |\n| Attention heads          | 16           | 12             | 24         |\n| Layer width              | 2048         | 1536           | 768        |\n| Feedforward hidden size  | 8192         | 6144           | 3072       |"
        },
        {
            "comment": "The code describes a Residual Embedding layer in Gato, which consists of residual networks with full-preactivation blocks based on the Version 2 ResNet architecture. The position encodings are divided into two phases - training and evaluation, and use patch encoding strategy similar to Vision Transformer model.",
            "location": "\"/media/root/Prima/works/gato/docs/src/README.md\":111-139",
            "content": "| Key/value size           | 128          | 128            | 32         |\n### Residual Embedding\n> Appendix C.2. Embedding Function\nThere are no mentions that how many residual networks must be stacked for token embeddings.<br>\nTherefore, I remain configurable in `GatoConfig`.\nWhatever how many residual layers are existing, full-preactivation is a key.\nThe blocks are consisted of:\n- Version 2 ResNet architecture (based on ResNet50V2)\n- GroupNorm (instead of LayerNorm)\n- GeLU (instead of ReLU)\n### Position Encodings\n> Appendix C.3. Position Encodings\n#### Patch Position Encodings\nLike [Vision Transformer (ViT)](https://github.com/google-research/vision_transformer) by Google, Gato takes the input images as raster-ordered 16x16 patches.<br>\nUnlike the Vision Transformer model, however, Gato divides its patch encoding strategy into 2 phases, training and evaluation.\nFor high-performance computation in TensorFlow, I have used the following expressions.\n$C$ and $R$ mean column and row-wise, and $F$ and $T$ mean `from` and `to` respectively."
        },
        {
            "comment": "The code defines position encodings for observation tokens, which can be text, image patches, or continuous/discrete values. If training, the values are randomly sampled from a range; otherwise, they're averaged between two ranges. These position encodings are then used to compute local observation position embeddings, which are added to the input embeddings for each token type.",
            "location": "\"/media/root/Prima/works/gato/docs/src/README.md\":141-175",
            "content": "$$\n\\begin{align}\n  v^R_F &= \\begin{bmatrix}\n    0 & 32 & 64 & 96\n  \\end{bmatrix} \\\\\n  v^R_T &= \\begin{bmatrix}\n    32 & 64 & 96 & 128\n  \\end{bmatrix} \\\\\n  v^C_F &= \\begin{bmatrix}\n    0 & 26 & 51 & 77 & 102\n  \\end{bmatrix} \\\\\n  v^C_T &= \\begin{bmatrix}\n    26 & 51 & 77 & 102 & 128\n  \\end{bmatrix} \\\\\n  \\\\\n  P_R &= \\begin{cases}\n    \\mathsf{if} \\ \\mathsf{training} & v^R_F + \\mathsf{uniform}(v^R_T - v^R_F) \\\\\n    \\mathsf{otherwise} & \\mathsf{round}(\\frac{v^R_F + v^R_T}{2})\n  \\end{cases} \\\\\n  P_C &= \\begin{cases}\n    \\mathsf{if} \\ \\mathsf{training} & v^C_F + \\mathsf{uniform}(v^C_T - v^C_F) \\\\\n    \\mathsf{otherwise} & \\mathsf{round}(\\frac{v^C_F + v^C_T}{2})\n  \\end{cases} \\\\\n  \\\\\n  E^R_P &= P_R \\cdot 1^{\\mathsf{T}}_C \\\\\n  E^C_P &= 1^{\\mathsf{T}}_R \\cdot P_C \\\\\n  \\\\\n  \\therefore E &= E_I + E^R_P + E^C_P\n\\end{align}\n$$\n#### Local Observation Position Encodings\nIn the definition of Appendix B., text tokens, image patch tokens, and discrete & continuous values are observation tokens<br>\nWhen Gato receives those values, they must be encoded with their own (local) time steps."
        },
        {
            "comment": "This code outlines the requirements for running the program, which includes installing TensorFlow version 2.11.0 or higher. It also mentions that the repository is still a work in progress and encourages contributions from developers. The project is licensed under the MIT license.",
            "location": "\"/media/root/Prima/works/gato/docs/src/README.md\":177-191",
            "content": "## Requirements\n```bash\npip install tensorflow>=2.11.0\n```\n## Contributing\nThis repository is still a work in progress.<br>\nCurrently, no downloads and no executables are provided.\nI welcome many contributors who can help.\n## License\nLicensed under the [MIT license](https://github.com/OrigamiDream/gato/blob/main/LICENSE)."
        }
    ]
}