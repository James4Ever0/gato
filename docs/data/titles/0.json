{
    "/README.md": "Gato: Unofficial Transformer Implementation",
    "/README.md:1-15": "Unofficial Gato Agent Implementation",
    "/README.md:112-140": "Residual Embedding Layer in Gato",
    "/README.md:142-176": "Position Encodings and Embeddings for Observation Tokens",
    "/README.md:16-35": "Understanding Gato Architecture and Tokenizing",
    "/README.md:178-192": "Title: \"TensorFlow 2.11.0+ Requirements\"",
    "/README.md:37-66": "Gato Model Instance Creation and Fake Input Generation",
    "/README.md:67-87": "GATO: Vision-Language Task Model Initialization",
    "/README.md:88-111": "Architecture Variants of Gato",
    "/gato/__init__.py": "Gato: Efficient Tensor Chaining",
    "/gato/config.py": "ML Model Initialization and Configuration",
    "/gato/config.py:1-34": "Gato Config: Versatile Model Parameters",
    "/gato/config.py:36-58": "Hyperparameters Setup for ML Model",
    "/gato/config.py:60-80": "Config Class for Gato Model",
    "/gato/models/__init__.py": "Gato Model Architecture",
    "/gato/models/__init__.py:1-22": "Gato Class Definition",
    "/gato/models/__init__.py:23-42": "Multi-Encoding Model Class",
    "/gato/models/__init__.py:43-68": "Gato Transformer Initialization",
    "/gato/models/__init__.py:69-96": "Transformer Class Initialization",
    "/gato/models/__init__.py:97-110": "Patch Embedding Class",
    "/gato/models/embedding.py": "Gato Embedding Models: Conv & Group Norm",
    "/gato/models/embedding.py:1-35": "PatchPositionEncoding Layer: Image Patch Encoding with Position Encodings",
    "/gato/models/embedding.py:125-144": "Building Gato Model Layers",
    "/gato/models/embedding.py:145-166": "ResNet Block for Token Embedding",
    "/gato/models/embedding.py:167-195": "LocalPositionEncoding Layer",
    "/gato/models/embedding.py:196-226": "Discrete Embedding Layer in Gato",
    "/gato/models/embedding.py:227-242": "Shared Embedding Layer for NLP",
    "/gato/models/embedding.py:37-55": "2D Position Embedding Model",
    "/gato/models/embedding.py:56-73": "Embedding Position Sampling",
    "/gato/models/embedding.py:74-95": "Residual Unit with Normalization",
    "/gato/models/embedding.py:96-123": "Residual Embedding Layer: GroupNorm and GELU",
    "/gato/models/tokenizers.py": "Mu-Law Continuous Value Tokenizer",
    "/gato/models/tokenizers.py:1-34": "Mu-Law Encoding Continuous Values Tokenizer",
    "/gato/models/tokenizers.py:35-47": "Continuous Value Tokenizer",
    "/gato/models/transformer.py": "Attention-Based Transformer Model for Text Analysis",
    "/gato/models/transformer.py:1-30": "Transformer Block Implementation",
    "/gato/models/transformer.py:31-47": "Attention-based Transformer Model with Layer Normalization",
    "/gato/models/transformer.py:48-70": "Pre-Normalized Transformer Block with Double Layer Normalization",
    "/setup.py": "Setup Script for Gato-TF Package"
}